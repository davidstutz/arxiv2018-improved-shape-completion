\section{Related Work}
\label{sec:related-work}

\subsection{3D Shape Completion and Single-View 3D Reconstruction}

In general, 3D shape completion is a special case of single-view 3D reconstruction where we assume point cloud observations to be available, \eg from laser-based sensors as on KITTI \citep{Geiger2012CVPR}.

\boldparagraph{3D Shape Completion}
%
Following \cite{Sung2015TG}, classical shape completion approaches can roughly be categorized into symmetry-based methods and data-driven methods. The former leverage observed symmetry to complete shapes; representative works include \citep{Thrun2005ICCV,Pauly2008TG,Zheng2010TG,Kroemer2012HUMANOIDS,Law2011CVIU}. Data-driven approaches, in contrast, as pioneered by \cite{Pauly2005SGP}, pose shape completion as retrieval and alignment problem. While \cite{Pauly2005SGP} allow shape deformations, \cite{Gupta2015CVPR}, use the iterative closest point (\ICP) algorithm \citep{Besl1992PAMI} for fitting rigid shapes. Subsequent work usually avoids explicit shape retrieval by learning a latent space of shapes \citep{Rock2015CVPR,Haene2014CVPR,Li2015CGF,Engelmann2016GCPR,Nan2012TG,Bao2013CVPR,Dame2013CVPR,Ngyuen2016CVPR}. Alignment is then formulated as optimization problem over the learned, low-dimensional latent space. For example, \cite{Bao2013CVPR} parameterize the shape prior through anchor points with respect to a mean shape, while \cite{Engelmann2016GCPR} and \cite{Dame2013CVPR} directly learn the latent space using principal component analysis and Gaussian process latent variable models \citep{Prisacariu2011CVPR}, respectively. In these cases, shapes are usually represented by signed distance functions (SDFs). \cite{Ngyuen2016CVPR} use 3DShapeNets \citep{Wu2015CVPR}, a deep belief network trained on occupancy grids, as shape prior. In general, data-driven approaches are applicable to real data assuming knowledge about the object category. However, inference involves a possibly complex optimization problem, which we avoid by amortizing, \ie, \emph{learning}, the inference procedure. Additionally, we also consider multiple object categories.

With the recent success of deep learning, several learning-based approaches have been proposed \citep{Firman2016CVPR,Smith2017ARXIV,Dai2017CVPRa,Sharma2016ARXIV,Rezende2016ARXIV,Fan2017CVPR,Riegler2017THREEDV,Han2017ICCV,Yang2017ARXIV,Yang2018ARXIVb}. Strictly speaking, these are data-driven, as well; however, shape retrieval and fitting are \emph{both} avoided by directly learning shape completion end-to-end, under full supervision -- usually on synthetic data from ShapeNet \citep{Chang2015ARXIV} or ModelNet \citep{Wu2015CVPR}. \cite{Riegler2017THREEDV} additionally leverage octrees to predict higher-resolution shapes; most other approaches use low resolution occupancy grids (\eg, $32^3$ voxels). Instead, \cite{Han2017ICCV} use a patch-based approach to obtain high-resolution results. In practice, however, full supervision is often not available; thus, existing models are primarily evaluated on synthetic datasets. In order to learn shape completion without full supervision, we utilize a learned shape prior to constrain the space of possible shapes. In addition, we use SDFs to obtain sub-voxel accuracy at higher resolutions (up to $48 \ntimes 108 \ntimes 48$ or $64^3$ voxels) without using patch-based refinement or octrees. We also consider significantly sparser observations.

\boldparagraph{Single-View 3D Reconstruction}
%
Single-view 3D reconstruction has received considerable attention over the last years; we refer to \citep{Oswal2013ISAMA} for an overview and focus on recent deep learning approaches, instead. Following \cite{Tulsiani2018ARXIV}, these can be categorized by the level of supervision. For example, \citep{Girdhar2016ECCV,Choy2016ECCV,Wu2016NIPS,Haene2017ARXIV} require full supervision, \ie, pairs of images and ground truth 3D shapes. These are generally derived synthetically. More recent work \citep{Yan2016NIPS,Tulsiani2017CVPR,Tulsiani2018ARXIV,Kato2017ARXIV,Lin2017ARXIV,Fan2017CVPR,Tatarchenko2017ICCV,Wu2016ECCV}, in contrast, self-supervise the problem by enforcing consistency across multiple input views. \cite{Tulsiani2018ARXIV}, for example, use a differentiable ray consistency loss; and in \citep{Yan2016NIPS,Kato2017ARXIV,Lin2017ARXIV}, differentiable rendering allows to define reconstruction losses on the images directly. While most of these approaches utilize occupancy grids, \cite{Fan2017CVPR} and \cite{Lin2017ARXIV} predict point clouds instead. \cite{Tatarchenko2017ICCV} use octrees to predict higher-resolution shapes. Instead of employing multiple views as weak supervision, however, we do not assume any additional views in our approach. Instead, knowledge about the object category is sufficient. In this context, concurrent work by \cite{Gwak2017ARXIV} is more related to ours: a set of reference shapes implicitly defines a prior of shapes which is enforced using an adversarial loss. In contrast, we use a denoising variational auto-encoder (\DVAE) \citep{Kingma2014ICLR,Im2017AAAI} to explicitly learn a prior for 3D shapes.

\subsection{Shape Models}

Shape models and priors found application in a wide variety of different tasks. In 3D reconstruction, in general, shape priors are commonly used to resolve ambiguities or specularities \citep{Dame2013CVPR,Guney2015CVPR,Kar2015CVPR}. Furthermore, pose estimation \citep{Sandhu2011PAMI,Sandhu2009CVPR,Prisacariu2013ACCV,Aubry2014CVPR}, tracking \citep{Ma2014ECCV,Leotta2009CVPR}, segmentation \citep{Sandhu2011PAMI,Sandhu2009CVPR,Prisacariu2013ACCV}, object detection \citep{Zia2013PAMI,Zia2014CVPR,Pepik2015CVPR,Song2014ECCV,Zheng2015GCPR} or recognition \citep{Lin2014ECCV} -- to name just a few -- have been shown to benefit from shape models. While most of these works use hand-crafted shape models, for example based on anchor points or part annotations \citep{Zia2013PAMI,Zia2014CVPR,Pepik2015CVPR,Lin2014ECCV}, recent work \citep{Liu2017ARXIV,Sharma2016ARXIV,Girdhar2016ECCV,Wu2016NIPS,Wu2015CVPR,Smith2017ARXIV,Nash2017SGP,Liu2017ARXIV} has shown that generative models such as \VAEs \citep{Kingma2014ICLR} or generative adversarial networks (\GANs) \citep{Goodfellow2014NIPS} allow to efficiently generate, manipulate and reason about 3D shapes. We use these more expressive models to obtain high-quality shape priors for various object categories.

\subsection{Amortized Inference}

To the best of our knowledge, the notion of amortized inference was introduced by \cite{Gersham2014COGSCI} and picked up repeatedly in different contexts \citep{Rezende2015ICML,Wang2016ARXIV,Ritchie2016ARXIV}. Generally, it describes the idea of \emph{learning to infer} (or learning to sample). We refer to \citep{Wang2016ARXIV} for a broader discussion of related work. In our context, a \VAE can be seen as specific example of learned variational inference \citep{Kingma2014ICLR,Rezende2015ICML}. Besides using a \VAE as shape prior, we also amortize the maximum likelihood problem corresponding to our 3D shape completion task.